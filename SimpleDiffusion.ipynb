{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple diffusion model(Denoising Diffusion Probabilistic Models)\n",
    "\n",
    "From paper https://arxiv.org/pdf/2006.11239.pdf\n",
    "\n",
    "Diffusion models are powerful generative models(can be applied to different types of data, this example focuses on images) able to generate high quality and diverse images, they are also compute intensive and generation time is slower than other types of models(that's why this example has not high quality images, this was created in a personal computer with small GPU with a small number of epochs and diffusion steps). Diffusion models are inspired by [non-equilibrium statistical physics](https://arxiv.org/pdf/1503.03585.pdf).\n",
    " Some famous large diffusion models are:\n",
    "* GLIDE\n",
    "* Dall-E(2)\n",
    "* Imagen\n",
    "* Stable Diffusion\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.vegait.co.uk/media/sgjf1oco/blog-stable-diffusion-_prateca-u-tekstu_gif.gif?width=746&height=297&mode=max\">\n",
    "\n",
    "Img src:[Prog. World](https://prog.world/sherudim-under-the-hood-of-stable-diffusion/)\n",
    "</center>\n",
    "\n",
    "Diffusion models generate data starting from noise and gradually denoising(removing the noise) one step at a time until the final image is obtained, this is known as reverse diffusion(in the following image right to left). Forward diffusion occurs at training time and implies the inverse: start from the final image and gradually add noise to it until pure noise is obtained(in the following image left to right).\n",
    "\n",
    "<center><img src=\"https://www.vegait.co.uk/media/lqdjatqq/blog-stable-diffusion_prateca-u-tekstu-4.jpg?width=751&height=299&mode=max\">\n",
    "\n",
    "Img src: [Nvidia developer blog](https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/)</center>\n",
    "\n",
    "Internally a neural network is trained to predict the noise so it can be subtracted in order to reverse the noising process. In the following image we have a noisy image, the neural network predicts the noise and then a portion(epsilon) of it is subtracted from the image to obtain a less noisy image, this process is repeated multiple steps to gradually remove noise.\n",
    "\n",
    "<center><img src=\"https://www.vegait.co.uk/media/qrof0p31/blog-stable-diffusion_prateca-u-tekstu-6.jpg?width=736&height=293&mode=max\">\n",
    "\n",
    "Img src: [Nvidia developer blog](https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/)\n",
    "</center>\n",
    "\n",
    "This notebook implements a simple and small diffusion model from the paper (Denoising Diffusion Probabilistic Models) described next:\n",
    "\n",
    "## Forward difussion\n",
    "\n",
    "Forward gradually converts an image to noise. Formally the forward diffusion process is a markov chain of T states(every step depends only in the previous step), the distribution $q(x_t|x_{t-1})$ (gaussian)of a variable $x$ in time \"$t$\" depends only(conditioned on) in the variable $x$ at time \"t-1\" $x_{t-1}$ (the variables $x$ are intermediate noisy images), at each step of the chain gaussian noise with variance $\\beta_{t}$ is added to the last variable at time $t-1$. There are multiple ways of defining the variances $\\beta$, in the literature these are called \"schedules\", in this notebook linear schedule is used.\n",
    "\n",
    "<center>\n",
    "<img src=\"./imgs/forward_diffusion.png\">\n",
    "\n",
    "img source: [theaisummer](https://theaisummer.com/diffusion-models/)\n",
    "</center>\n",
    "\n",
    "A remarkable property is that with some algebra and probability theory(reparametrization trick)the markov chain(forward diffusion) can be manipulated to obtain the gaussian distrubution at time $t$ $q(x_t|x_0)$ only from the original image $x_0$, this allows equivalent direct sampling in one single step. If we remember the $x_t$ in the markov chain are intermediate images, this means we can generate intermediate noisy images only from the initial image $x_0$ and the variance schedule.\n",
    "We introduce the variables $\n",
    "\\alpha_t$ and get:\n",
    "\n",
    "<center>\n",
    "<img src=\"./imgs/alphat.png\">\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<img src=\"./imgs/alphabart.png\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"./imgs/equivalent_diffusion.png\"></center>\n",
    "\n",
    "In the paper it is shown that with a large enough $T$ (number of diffusion steps) and a well defined variance schedule, the last noisy image is a sample from a standard gaussian distribution with mean 0 and variance 1, this allows to then use a standard gaussian as starting point of the generation(reverse diffusion).\n",
    "\n",
    "## Reverse Diffusion\n",
    "\n",
    "Reverse diffusion gradually converts pure gaussian noise to a final image using the reverse markov chain $q(x_{t-1}|q_{t})$(the papers shows that the variances beta from the schedule should be small for this to be gaussian). \n",
    "<center><img src=\"./imgs/backward_diffusion.png\"></center>\n",
    "\n",
    "Contrary to the forward markov chain, the distribution in the backward markov chain cannot be analytically calculated, so it is needed to find an approximation, and that's where a neural network comes into play, it will be used to learn a distribution(with parameters $\\theta$) $p_\\theta(x_{t-1}|x_t)$ that approximates $q(x_{t-1}|x_t)$. Since $q$ is gaussian  what the neural network has to learn is the mean and variance of the distribution as a function of $x_t$. In this example the variance is fixed(via  the schedule) instead of learned, and the paper shows that the mean can be defined in terms of the alphas $\\alpha$ and an approximation of the noise $\\epsilon(x_t)$. So in summary the neural nettwork has to learn to predict the noise from a given image $x_t$ and a portion of it(weighted in terms of the variances) is calculated and then this is subtracted from the image $x_t$, this  gives us the following expression for the mean of the gaussian:\n",
    "\n",
    "<center><img src=\"./imgs/diffusion_mean.png\"></center>\n",
    "\n",
    "## Loss Function and Training\n",
    "\n",
    "The training procedure is similar to the procedure used by autoencoders by optimizing the usual variational evidence lower bound (ELBO) on the negative log likelihood with the KL-divergence between the markov chain real q and predicted q gaussian distributions. \n",
    "<center>\n",
    "<img src=\"./imgs/diffusion_ELBO.png\">\n",
    "\n",
    "<img src=\"./imgs/diffusion_KL.png\">\n",
    "</center>\n",
    "\n",
    "More details can be found in the paper but it shows that training is equivalent to minimizing the l2 distance between real gaussian noise and predicted noise(this is what will  go into the code) for a step t:\n",
    "\n",
    "<center><img src=\"./imgs/diffusion_l2_distance.png\"></center>\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "This theory allows us to define 2 simple and short algorithms, one for training and one for sampling generated images.\n",
    "\n",
    "### Algorithm 1(Training)\n",
    "\n",
    "<center><img src=\"./imgs/diffusion_algorithm1.png\"></center>\n",
    "\n",
    "In simple words:\n",
    "1. Pick a random image from the dataset\n",
    "2. Sample a random t(step number) from 1 to T with equal probability.\n",
    "3. Generate random noise epsilon\n",
    "4. Use the noise approximator(neural network) to predict the noise of an intermediate image at timestep t\n",
    "5. Calculate the loss(MSE) between the real noise(step 3) and predicted noise(step 4)\n",
    "6. Calculate gradient of the loss with respect to the neural network parameters and perform gradient descent step\n",
    "7. Go  to step 1 until convergence(or a fixed number of iterations)\n",
    "\n",
    "\n",
    "### Algorithm 2(Sampling)\n",
    "This implements the backward diffusion, a neural network(U-net in this example) is used to predict the noise, alphas(from the variances) calculated during training are used.\n",
    "\n",
    "<center><img src=\"./imgs/diffusion_sampling.png\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PLOT_PICTURES = 5\n",
    "DEVICE = \"cuda:0\"\n",
    "#DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize subtracting the mean 0.5 and dividing by the standard deviawtion 0.5\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(root=\"./diffusion_data\",train=True,download=True,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size = BATCH_SIZE, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for i in range(NUM_PLOT_PICTURES):\n",
    "    img = train_set[i][0]\n",
    "    ax = plt.subplot(1,NUM_PLOT_PICTURES,i+1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.imshow(img.cpu().squeeze().numpy(), cmap=\"gist_gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGES_CHANNELS = 1\n",
    "IMAGES_SIZE = 28\n",
    "\n",
    "def plot_sample(model):\n",
    "    with torch.no_grad():\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "\n",
    "        for i in range(NUM_PLOT_PICTURES):\n",
    "            sample = model.forward_diffusion((IMAGES_CHANNELS, IMAGES_SIZE, IMAGES_SIZE)).cpu()\n",
    "            plt.subplot(1, NUM_PLOT_PICTURES, i+1)\n",
    "            plt.imshow(sample.reshape(IMAGES_SIZE,IMAGES_SIZE),cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the variance schedule and calculate all dependant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beta_t': tensor([1.0000e-04, 1.1990e-04, 1.3980e-04,  ..., 1.9960e-02, 1.9980e-02,\n",
       "         2.0000e-02]),\n",
       " 'alpha_t': tensor([0.9999, 0.9999, 0.9999,  ..., 0.9800, 0.9800, 0.9800]),\n",
       " 'sqrt_beta_t': tensor([0.0100, 0.0109, 0.0118,  ..., 0.1413, 0.1414, 0.1414]),\n",
       " 'log_alpha_t': tensor([-0.0001, -0.0001, -0.0001,  ..., -0.0202, -0.0202, -0.0202]),\n",
       " 'log_alpha_bar_t': tensor([-1.0002e-04, -2.1995e-04, -3.5974e-04,  ..., -1.0087e+01,\n",
       "         -1.0108e+01, -1.0128e+01]),\n",
       " 'alpha_bar_t': tensor([9.9990e-01, 9.9978e-01, 9.9964e-01,  ..., 4.1599e-05, 4.0767e-05,\n",
       "         3.9952e-05]),\n",
       " 'sqrt_alpha_bar_t': tensor([0.9999, 0.9999, 0.9998,  ..., 0.0064, 0.0064, 0.0063]),\n",
       " 'one_over_sqrt_alpha_t': tensor([1.0001, 1.0001, 1.0001,  ..., 1.0101, 1.0101, 1.0102]),\n",
       " 'one_minus_alpha_bar_t': tensor([1.0002e-04, 2.1994e-04, 3.5965e-04,  ..., 9.9996e-01, 9.9996e-01,\n",
       "         9.9996e-01]),\n",
       " 'one_over_sqrt_alpha_bar_t': tensor([  1.0001,   1.0001,   1.0002,  ..., 155.0461, 156.6187, 158.2087]),\n",
       " 'sqrt_one_minus_alpha_bar_t': tensor([0.0100, 0.0148, 0.0190,  ..., 1.0000, 1.0000, 1.0000]),\n",
       " 'one_minus_alpha_t_over_sqrt_omab': tensor([0.0100, 0.0081, 0.0074,  ..., 0.0200, 0.0200, 0.0200])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def ddpm_schedule(beta1, beta2, T):\n",
    "    \"\"\"create the schedule for the betas(variances) lineraly\n",
    "       in the paper: variance increases with timesteps\n",
    "    \"\"\"\n",
    "    beta_t = torch.linspace(beta1,beta2,T,dtype=torch.float32) # the variance schedule for the markov chain\n",
    "    beta_t = (beta2-beta1) * torch.arange(0,T+1,dtype=torch.float32)/T+beta1\n",
    "\n",
    "    # calculate derived quantities from beta\n",
    "    alpha_t = 1 - beta_t # from equation 2 αt:= 1 − βt\n",
    "    sqrt_beta_t  =torch.sqrt(beta_t) # step 4 of algorithm2 σt\n",
    "    ## work in logarithmic space for stability but transform back using exp()(product becomes sum)\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    log_alpha_bar_t = torch.cumsum(log_alpha_t,dim=0)\n",
    "    alpha_bar_t = torch.exp(log_alpha_bar_t) # α¯t:= cumprod(αt),\n",
    "\n",
    "    sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t) # used for the mean of the gaussian in the conditional distribution(aquation 4) for xt conditioned on x0\n",
    "    one_over_sqrt_alpha_t = 1/torch.sqrt(alpha_t) # from equation 10 and step 4 of algorithm 2\n",
    "    one_minus_alpha_bar_t  = 1 - alpha_bar_t # used for the var of the gaussian in the conditional distribution(equation 4)\n",
    "    one_over_sqrt_alpha_bar_t = 1/torch.sqrt(alpha_bar_t) # from equation 9 of the paper\n",
    "    sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - alpha_bar_t) # from equation 9 of the paper\n",
    "    one_minus_alpha_t_over_sqrt_omab = (1 - alpha_t)/sqrt_one_minus_alpha_bar_t # from step 4, algorithm 2(sampling)\n",
    "\n",
    "    return {\n",
    "        \"beta_t\": beta_t,\n",
    "        \"alpha_t\": alpha_t,\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,\n",
    "        \"log_alpha_t\": log_alpha_t,\n",
    "        \"log_alpha_bar_t\": log_alpha_bar_t,\n",
    "        \"alpha_bar_t\": alpha_bar_t,\n",
    "        \"sqrt_alpha_bar_t\": sqrt_alpha_bar_t,\n",
    "        \"one_over_sqrt_alpha_t\": one_over_sqrt_alpha_t,\n",
    "        \"one_minus_alpha_bar_t\": one_minus_alpha_bar_t,\n",
    "        \"one_over_sqrt_alpha_bar_t\": one_over_sqrt_alpha_bar_t,\n",
    "        \"sqrt_one_minus_alpha_bar_t\": sqrt_one_minus_alpha_bar_t,\n",
    "        \"one_minus_alpha_t_over_sqrt_omab\": one_minus_alpha_t_over_sqrt_omab\n",
    "    }\n",
    "\n",
    "ddpm_schedule(1e-4,0.02,1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "### Noise predictor model\n",
    "This is a simplified U-net, the papers used a much more complex and bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_block(input_channels,output_channels):\n",
    "    \"\"\"Will be used to create the conv2d UNET\n",
    "       this one is simplified, the paper uses skip-connections among others\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(input_channels,output_channels,7,padding=3),\n",
    "        nn.BatchNorm2d(output_channels),\n",
    "        nn.LeakyReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"The unet for the diffusion model\"\"\"\n",
    "    def __init__(self,num_channels):\n",
    "        super(SimpleUNet,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            model_block(num_channels,64),\n",
    "            model_block(64,128),\n",
    "            model_block(128,256),\n",
    "            model_block(256,512),\n",
    "            model_block(512,256),\n",
    "            model_block(256,128),\n",
    "            model_block(128,64),\n",
    "            nn.Conv2d(64, num_channels,3,padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        return self.network(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion model\n",
    "\n",
    "This used both the scheduler and the U-net predictor model to create a complete diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    \"\"\"the denoising diffusion probabilistic model\"\"\"\n",
    "    def __init__(self,epsilon_theta,beta1,beta2,n_T):\n",
    "        super(DDPM,self).__init__()\n",
    "        self.epsilon_theta = epsilon_theta # the noise predictor model parametrized by theta(step 5 of altorighm 1), this is to be trained\n",
    "        self.n_T = n_T\n",
    "\n",
    "        # using buffers allows to access every item in the dictionary by name with dot operator for the class\n",
    "        ## for example self.key and let pytorch perform device placement\n",
    "        ## plus, these objects are not considered parameters(do not require gradients)\n",
    "        for key,value in ddpm_schedule(beta1=beta1,beta2=beta2,T=n_T).items():\n",
    "            self.register_buffer(key,value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # algorithm 1(training) from paper\n",
    "        ## the x0 from the paper(step1 of algorithm 1) are the x batch\n",
    "        t = torch.randint(0,self.n_T,(x.shape[0],)).to(device) #sample timestep(line 3 of algorithm 1), minibatch of the same size of images\n",
    "        epsilon = torch.randn_like(x) # sample gaussian noise with same dimensions as the batch x(step 4 of algorithm 1)\n",
    "\n",
    "        # the following samples a x at timestep t for every image in the batch x\n",
    "        ## this will be used to predict the noise of the image x_t\n",
    "        ## this is based on equation 4 and step 5 of algorithm 1(argument of epsilon theta)\n",
    "        ## this is an intermediary image for the diffusion process\n",
    "        x_t = (self.sqrt_alpha_bar_t[t,None,None,None]*x + self.sqrt_one_minus_alpha_bar_t[t,None,None,None]*epsilon)  \n",
    "        \n",
    "        #  return the real noise and the noise prediction\n",
    "        return epsilon, self.epsilon_theta(x_t,t/self.n_T)\n",
    "    \n",
    "    def forward_diffusion(self,size):\n",
    "        # algorithm 2(sampling) from paper\n",
    "        x_t = torch.randn(1,*size).to(device) # start generation from random noise\n",
    "\n",
    "        for t in range(self.n_T,0,-1):\n",
    "            # step 3 of altorighm 2\n",
    "            if t > 1:\n",
    "                z = torch.randn(1,*size).to(device)\n",
    "            else:\n",
    "                z = 0\n",
    "            # step 4 of algorithm 2\n",
    "            x_t = (\n",
    "                self.one_over_sqrt_alpha_t[t]*(x_t-self.one_minus_alpha_t_over_sqrt_omab[t]*self.epsilon_theta(x_t,t/self.n_T)) + \n",
    "                self.sqrt_beta_t[t]*z\n",
    "            )\n",
    "\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DDPM(epsilon_theta=SimpleUNet(1), #one input channel, this is the trainable model\n",
    "                       beta1=1e-4,#initial variance\n",
    "                       beta2=0.02, # final variance,\n",
    "                       n_T=1000 #number of  diffusion steps\n",
    "                       ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(diffusion_model.parameters(),lr=2e-4)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step 1, loss:1.15420\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training:\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    diffusion_model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, data  in enumerate(train_loader):\n",
    "        X,Y = data\n",
    "        X = X.to(device)\n",
    "\n",
    "        # train step\n",
    "        optimizer.zero_grad()\n",
    "        # forward diffusion of the DDPM model\n",
    "        ## first get the  sample noise epsilon, and predicted  noise epsilon theta\n",
    "        epsilon, epsilon_theta = diffusion_model(X) \n",
    "        ## get the loss from real and predicted noise\n",
    "        step_loss = loss(epsilon,epsilon_theta)\n",
    "        ## get gradients with backpropagation and perform gradient descent step\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += step_loss.item()\n",
    "        #print(running_loss)\n",
    "        #progress_bar.set_description(f\"epoch:{epoch}, loss:{running_loss/(step+1):.5f}\")\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"epoch:{epoch+1} step {step+1}, loss:{running_loss/(step+1):.5f}\")\n",
    "    #if epoch % 5 == 0:\n",
    "    plot_sample(diffusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311_torch113_tf212",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
