{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b703be",
   "metadata": {},
   "source": [
    "# VQ-GAN+CLIP \n",
    "\n",
    "***Author***: Luis Leal\n",
    "\n",
    "This notebook implements the vqgan+clip architecture used to generate images conditioned on a text prompt(the paper says it can be used for image editing too and the only change is to start with the image to be edited instead of a random initial image). \n",
    "\n",
    "This notebook is based on the paper [VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance](https://arxiv.org/abs/2204.08583) and the idea is to combine 2 models initially developed separetly.\n",
    "\n",
    "* **CLIP(contrastive language-image pre-training)**: originally developed to find the text that best matches an image(and visceversa) via contrastive training by embedding both images and text to a common latent space where semantics are maintained(an image of a dog will have an embedding similar to the text 'dog').\n",
    "    <img src=\"CLIP_diagram.png\">\n",
    "    \n",
    "    Paper: \n",
    "    [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)\n",
    "* **VQ-GAN(vector quantized GAN)**:  vector quantized  GAN, an improvement of the [vector quantized variational autoencoder](https://arxiv.org/pdf/1711.00937.pdf) where the  latent space is discretized via a codebook in order to learn discrete neural representations(instead of an infinite space) for modalities where it makes sense by nature, for example text.In VQ-GAN images are a composition of codebook vectors(we can think of the codebook as a toolbox of a finite set of visual features) and generation of images is performed via a transformer that models sequences of codebook vectors. This architecture is used to generate images but not conditioned on any text, as original GANS.\n",
    "<img src=\"VQGAN_diagram.png\">\n",
    "\n",
    "Paper: \n",
    "    [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/pdf/2012.09841.pdf)\n",
    "\n",
    "**The high level idea of this mix is:**\n",
    "\n",
    "Given a text prompt(and some additional text context described later) the VQGAN will generate an image(initially random) and CLIP will evaluate how good the image corresponds to the text prompt via cosine similarity, this evaluation corresponds to the loss function of the model and the goal is to maximize this similarity(or minimize the negative of it).\n",
    "\n",
    "The VQGAN paper uses a Transformer for generating new images because the transformer models sequences of codebook vectors, for this vqgan+clip architecture the transformer is not used at all because we replace the sequence of codebook vectors by vectors obtained via gradient descent by calculating gradients of the CLIP loss function with respect to these vectors. In other words: instead of the transformer giving us the latent vectors to input to the VQGAN decoder we input vectors obtained via CLIP comparisons between current image embedding and text prompt embedding so CLIP guides the generation.\n",
    "\n",
    "**NOTE**: this notebook uses pretrained models for both CLIP and VQ-GAN, they will not be trained and will only download them from github."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de250fe",
   "metadata": {},
   "source": [
    "!git clone https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c17334b1",
   "metadata": {},
   "source": [
    "!git clone https://github.com/CompVis/taming-transformers taming_transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "058b2d54",
   "metadata": {},
   "source": [
    "!conda install -y -c conda-forge omegaconf\n",
    "!conda install -y -c conda-forge pytorch-lightning\n",
    "!conda install -y -c conda-forge einops\n",
    "!conda install -y -c conda-forge ftfy\n",
    "!conda install -y -c conda-forge regex\n",
    "!conda install -y -c conda-forge imageio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931d496",
   "metadata": {},
   "source": [
    "**note**: some of the previous installs messed my pytorch installation and cuda did not work, had to reinstall"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05551e3b",
   "metadata": {},
   "source": [
    "!pip3 install torch torchvision torchaudio --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f50b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"taming_transformers/\")\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import imageio\n",
    "import  math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as tf\n",
    "\n",
    "from  taming_transformers.taming.models.vqgan import VQModel\n",
    "\n",
    "import PIL\n",
    "\n",
    "import yaml\n",
    "from omegaconf import  OmegaConf\n",
    "\n",
    "from CLIP import clip\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f869c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_from_tensor(tensor):\n",
    "    \"\"\"show image\"\"\"\n",
    "    img = tensor.clone()\n",
    "    img = img.mul(255).byte()\n",
    "    img = img.cpu().numpy().transpose((1,2,0))\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def norm_data(data):\n",
    "    return (data.clip(-1,1)+1)/2 # range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_MODELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59022fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9906f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5\n",
    "BATCH_SIZE = 1\n",
    "WEIGHT_DECAY = 0.1 # for optimizer regularization\n",
    "NOISE_FACTOR = 0.22 # for inject noise when creating crops\n",
    "\n",
    "TOTAL_ITERATIONS = 400\n",
    "IMAGE_SHAPE = [400, 400, 3] # height, widgth, channel\n",
    "height, width, channels = IMAGE_SHAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c36a",
   "metadata": {},
   "source": [
    "## Setup CLIP\n",
    "\n",
    "Used to encode both text prompts, and generated images to compare their similarity.\n",
    "Goal is to make the generated images to become closer and closer to the text prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56add266",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipmodel,_= clip.load(\"ViT-B/32\",jit = False)  #download pre-trained clip(ViT is visual transformer)\n",
    "clipmodel = clipmodel.to(DEVICE)\n",
    "clipmodel.eval() # will not train CLIP just do inference\n",
    "\n",
    "print(\"avaliable CLIP modelsL\",clip.available_models())\n",
    "print(\"CLIP model image resolution:\",clipmodel.visual.input_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630c4d0",
   "metadata": {},
   "source": [
    "### Setup VQ-GAN(from Taiming Transformers)\n",
    "\n",
    "The transformer part of the architecture is not used because in the original vq-gan paper the transformer generates a sequence of zq vectors(from the codebook) to feed to the decoder, but in this vqgan+clip architecture  CLIP determines the zq vectors to use(before quantization) based on the text prompt, by minimizing the CLIP loss the correct zq vectors are used(instead of generated by the transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_MODELS:\n",
    "\n",
    "    URL = \"https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1\"\n",
    "    target_dir =  \"./taming_transformers/models/vqgan_imagenet_f16_16384/checkpoints/\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    open(f\"{target_dir}last.ckpt\", \"wb\").write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_MODELS:\n",
    "\n",
    "    URL = \"https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1\"\n",
    "    target_dir =  \"./taming_transformers/models/vqgan_imagenet_f16_16384/configs/\"\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    open(f\"{target_dir}model.yaml\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9965f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path, display=False):\n",
    "    config_data = OmegaConf.load(config_path)\n",
    "    \n",
    "    if display:\n",
    "        print(yaml.dump(OmegaConf.to_container(config_data)))\n",
    "    \n",
    "    return config_data\n",
    "\n",
    "def load_vqgan(config, checkpoint_path=None):\n",
    "    model = VQModel(**config.model.params)\n",
    "    \n",
    "    if checkpoint_path is not None:\n",
    "        state_dict = torch.load(checkpoint_path,map_location=\"cpu\")[\"state_dict\"]\n",
    "        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def generate(model,z_hat):\n",
    "    \"\"\"\n",
    "    it passes the latent vector z_hat for current  candidate generation\n",
    "    through the quantize + decode step of the vqgan to get a new image.\n",
    "    \n",
    "    In the original vqgan this z_hat is obtained:\n",
    "    \n",
    "    during training this is the output of the CNN Encoder E for training images\n",
    "    during inference/generation the tranformer generates a sequence of quantized vectors so z_hat is not \n",
    "    used because it's quantized version is generated by a Transformer.\n",
    "    \n",
    "    in this vqgan+clip paper this z_hat is the candiate image generated by backpropagating through clip,\n",
    "    so we can say we are replacing the CNN Encoder of the original VQGAN with our CLIP guided generations.\n",
    "    \n",
    "    Instead of generating with the transformer we will: generate a z_hat guided by CLIP, cuantize it and decode it.\n",
    "    \n",
    "    The vqgan+clip architecture differentiates the CLIP loss with respect to the z_hat vector(as parameter)\n",
    "    meaning it finds the z_hat that minimizes the CLIP loss, so z_hat has to be a parameters tensor\n",
    "    updated via training.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # uses the CNN Decoder(in the vqgan paper called G) to generate a new image\n",
    "    # from a cuantized latent z hat\n",
    "    #print(z_hat.shape)\n",
    "    zquant, emb_loss, info = model.quantize(z_hat)\n",
    "    z_q = model.post_quant_conv(zquant) \n",
    "    new = model.decoder(z_q) \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqgan_config = load_config(\"./taming_transformers/models/vqgan_imagenet_f16_16384/configs/model.yaml\",\n",
    "                          display = True\n",
    "                          )\n",
    "vqgan_model = load_vqgan(vqgan_config, \n",
    "                        checkpoint_path=\"./taming_transformers/models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\",\n",
    "                        ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231b167",
   "metadata": {},
   "source": [
    "Working with z of shape (1, 256, 16, 16) means (1, channels, patch height, patch width) because it works in patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756460b2",
   "metadata": {},
   "source": [
    "### Declare optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d31c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The parameters that we will be optimizing the CLIP loss with respect to.\n",
    "    These are latent vectors for the candidate images(before quantization in vqgan, called z_hat in the paper)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Parameters, self).__init__()\n",
    "        self.data = 0.5*torch.randn(BATCH_SIZE, 256, \n",
    "                                height//16, width//16).to(DEVICE) #1x256x14x15 (225/16, 400/16)\n",
    "        self.data = torch.nn.Parameter(torch.sin(self.data)) #positional embedding for the transformer?\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.data\n",
    "    \n",
    "def init_params():\n",
    "    params = Parameters().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW([{\"params\":[params.data], \"lr\":LEARNING_RATE}],\n",
    "                                weight_decay=WEIGHT_DECAY\n",
    "                                )\n",
    "    \n",
    "    return params, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10d2f6",
   "metadata": {},
   "source": [
    "### Preprocessing:  \n",
    "Encoding prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using pretrained models we need to match their normalization statistics  means and stds\n",
    "normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                             (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeText(text):\n",
    "    \"\"\"Create tokens consistent with CLIP\"\"\"\n",
    "    new_text = clip.tokenize(text).to(DEVICE)\n",
    "    text_encoding = clipmodel.encode_text(new_text).detach().clone()\n",
    "    return text_encoding\n",
    "\n",
    "def createEncodings(include, exclude, extras):\n",
    "    \"\"\"\n",
    "    include: what images we want in the space(list)\n",
    "    exclude: what we don't want in the space(comma separated string)\n",
    "    extras: additional context or characteristics(comma separated string)\n",
    "    \"\"\"\n",
    "    \n",
    "    include_encodings = []\n",
    "    \n",
    "    \n",
    "    for text in include:\n",
    "        include_encodings.append(encodeText(text))\n",
    "        \n",
    "    exclude_encodings = encodeText(exclude) if exclude != \"\" else 0\n",
    "    extras_encodings = encodeText(extras) if extras != \"\" else 0\n",
    "    \n",
    "    return include_encodings, exclude_encodings, extras_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866473cb",
   "metadata": {},
   "source": [
    "### Preprocessing:  \n",
    "Image preprocessing\n",
    "* first augmentations are applied to single image\n",
    "* from augmentations output different crops are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_transform = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomAffine(30, (0.2, 0.2), fill=0)\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, optimizer = init_params()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(params().shape)\n",
    "    img  = norm_data(generate(vqgan_model, params()).cpu()) # 1 x 3 x 224 x 400\n",
    "    print(\"img dimensions:\", img.shape)\n",
    "    show_from_tensor(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebc3f3",
   "metadata": {},
   "source": [
    "create crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec074f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crops(img, num_crops = 30):\n",
    "    p =  height//2\n",
    "    img = torch.nn.functional.pad(img,\n",
    "                                  (p,p,p,p),\n",
    "                                  mode=\"constant\",value=0) # 1 x 3 x 448 x 624 (adding 112*2 on all sides)\n",
    "    img = augment_transform(img) \n",
    "    \n",
    "    crop_set = []\n",
    "    for crop in range(num_crops):\n",
    "        gap1=int(torch.normal(1.0,0.5,()).clip(0.2,1.5)*height)\n",
    "        gap2 =int(torch.normal(1.0, 0.5, ()).clip(0.2,1.5)*height)\n",
    "        offsetx = torch.randint(0, int(height*2-gap1),())\n",
    "        offsety = torch.randint(0, int(height*2-gap1),())\n",
    "        \n",
    "        crop = img[:,:, offsetx:offsetx+gap2, offsety:offsety+gap2]\n",
    "        \n",
    "        crop = torch.nn.functional.interpolate(crop, (224, 224),\n",
    "                                              mode=\"bilinear\", align_corners=True)\n",
    "        crop_set.append(crop)\n",
    "        \n",
    "    img_crops = torch.cat(crop_set,0) ## num_crops x 3 x 224 x 224\n",
    "    img_crops = img_crops + NOISE_FACTOR*torch.randn_like(img_crops, requires_grad = False)\n",
    "    \n",
    "    return img_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated(params, show_crop):\n",
    "    with torch.no_grad():\n",
    "        generated = generate(vqgan_model,params())\n",
    "        \n",
    "        if show_crop:\n",
    "            print(\"Augmented cropped example:\")\n",
    "            aug_gen = generated.float() # 1 x 3 x height x width\n",
    "            aug_gen = create_crops(aug_gen, num_crops=1)\n",
    "            aug_gen_norm = norm_data(aug_gen[0])\n",
    "            show_from_tensor(aug_gen_norm)\n",
    "            \n",
    "        print(\"Generated:\")\n",
    "        latest_gen = norm_data(generated.cpu()) # 1 x 3 x height x width\n",
    "        show_from_tensor(latest_gen[0])\n",
    "        \n",
    "    return latest_gen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bea4f",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Optimize the parameters so the generated image matches the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_result(params, prompt, include_enc, exclude_enc, extras_enc):\n",
    "    # importance of the encodings(maybe should move this to function params or global constants?)\n",
    "    alpha = 1.0 # importance of the include encodings\n",
    "    beta = 0.5 #importance of the exclude encodings\n",
    "    \n",
    "    \"\"\"\n",
    "    to calculate the loss we compare image and text encodings\n",
    "    we need a generated image to encode from current params\n",
    "    \"\"\"\n",
    "    # generate a candidate image with vqgan\n",
    "    output = generate(vqgan_model, params())\n",
    "    output = norm_data(output)\n",
    "    output = create_crops(output)\n",
    "    output = normalize(output) # 30 x 3 x 224 x 224\n",
    "    # encode the generated image with clip\n",
    "    image_encoded = clipmodel.encode_image(output) # 30 x 512(for each of the 30 crops return a z vector of 512)\n",
    "    \n",
    "    # text encoding\n",
    "    final_encoding = w1*prompt + w2*extras_enc\n",
    "    final_text_include_enc = final_encoding/final_encoding.norm(dim=-1, keepdim=True) # 1x512\n",
    "    final_text_exclude_enc = exclude_enc\n",
    "    \n",
    "    loss = torch.cosine_similarity(final_text_include_enc, image_encoded, -1) #30\n",
    "    penalize_loss = torch.cosine_similarity(final_text_exclude_enc,image_encoded)\n",
    "    \n",
    "    # mazimize the similarity between include+extras  and image encodings(via minimze the negative)\n",
    "    # and minimize exclude and image encodings\n",
    "    final_loss = -alpha*loss + beta*penalize_loss\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def optimize(params, optimizer, prompt, include_enc, exclude_enc, extras_enc):\n",
    "    loss = optimize_result(params, prompt, include_enc, exclude_enc, extras_enc).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbc9e3",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(include_text, exclude_text, extras_text,\n",
    "          params, optimizer, show_crop = False, capture_gen_every = -1):\n",
    "    \"\"\"\n",
    "    capture_gen_every  = capture output frequency, -1 means only the last one\n",
    "    \"\"\"\n",
    "    capture_gen_every = TOTAL_ITERATIONS - 1 if capture_gen_every == -1 else capture_gen_every\n",
    "    \n",
    "    result_img = [] # store images\n",
    "    result_z = [] # store latent space/parameters\n",
    "    \n",
    "    \n",
    "    include_enc, exclude_enc, extras_enc = createEncodings(include_text,exclude_text,extras_text)\n",
    "    \n",
    "    for i,prompt in enumerate(include_enc):\n",
    "        print(f\"\\n-------------------STARTING: {include_text[i]} -------------------------\")\n",
    "        iteration = 0\n",
    "        params, optimizer = init_params()# 1 x 256 x height/16 x width /16\n",
    "        \n",
    "        for iteration in range(TOTAL_ITERATIONS):\n",
    "            loss = optimize(params, optimizer, prompt, include_enc, exclude_enc, extras_enc)\n",
    "            \n",
    "            if iteration > 0 and iteration%capture_gen_every==0:\n",
    "                new_img = show_generated(params, show_crop)\n",
    "                result_img.append(new_img)\n",
    "                result_z.append(params().detach().cpu().numpy())\n",
    "                \n",
    "                print(f\"prompt:{include_text[i]}\")\n",
    "                print(f\"loss:{loss.item()} \\niteration:{iteration}\")\n",
    "                \n",
    "                \n",
    "            iteration+=1\n",
    "        print(f\"-------------------ENDING: {include_text[i]} -------------------------\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return result_img, result_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a004f2de",
   "metadata": {},
   "source": [
    "include = [\"sketch of a lady\", \"sketch of a man and a horse\"]\n",
    "exclude = \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"oil painting\"\n",
    "\n",
    "w1=1\n",
    "w2=1\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "988fe769",
   "metadata": {},
   "source": [
    "print(len(result_imgs), len(result_z))\n",
    "print(result_imgs[0].shape, result_z[0].shape)\n",
    "print(result_imgs[0].min().item(), result_z[0].max().item())\n",
    "print(result_z[0].min().item(), result_z[0].max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd598df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = ['A forest with purple trees', \n",
    "           #'one  big elephant at the top of a mountain',\n",
    "           'A painting of a pineapple in a bowl',\n",
    "          \"a wolf looking at the stars at night\",\n",
    "           #\"a futuristic city in synthwave style\"\n",
    "          ] #[\"a boy in a mountain playing guitar for his dog\"]\n",
    "TOTAL_ITERATIONS = 200\n",
    "exclude =  \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"\"#\"watercolor paper texture\"\n",
    "\n",
    "w1=1\n",
    "w2=0.5\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True,\n",
    "                              capture_gen_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb876601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result_imgs), len(result_z))\n",
    "print(result_imgs[0].shape, result_z[0].shape)\n",
    "print(result_imgs[0].min().item(), result_z[0].max().item())\n",
    "print(result_z[0].min().item(), result_z[0].max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77393372",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e90868",
   "metadata": {},
   "source": [
    "### Create animations for interpolations of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(res_z_list, duration_list):\n",
    "    \"\"\"\n",
    "    for very image in res_z_list we pass a duration(seconds) in duration_list\n",
    "    \"\"\"\n",
    "    gen_img_list = []\n",
    "    \n",
    "    fps = 25 # frames per second\n",
    "    \n",
    "    for idx, (z,duration) in enumerate(zip(res_z_list, duration_list)):\n",
    "        torch.cuda.empty_cache()\n",
    "        num_steps = int(duration*fps) # number of frames\n",
    "        z1 = z\n",
    "        \n",
    "        # the modular division allows to interpolate from last image to first \n",
    "        z2 = res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x (height/16) x (width/16)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            #make it a bit interesting: interpolation is not linear\n",
    "            ## faster in the midle and slower at the end periodically\n",
    "            alpha = math.sin(1.5*step/num_steps)**6 \n",
    "            z_new = alpha * z2 + (1-alpha) * z1  #common interpolation formula\n",
    "            \n",
    "            new_gen = norm_data(generate(vqgan_model,\n",
    "                               torch.Tensor(z_new).to(DEVICE)).cpu())[0] ## 3 x height x width\n",
    "            new_img = transforms.ToPILImage(mode=\"RGB\")(new_gen)\n",
    "            gen_img_list.append(new_img)\n",
    "            \n",
    "    return gen_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e75d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "durations = [1]*len(result_z)\n",
    "interpolation_results = interpolate(result_z,durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = \"./outputs/output.mp4\"\n",
    "writer = imageio.get_writer(output_video_path,fps=25)\n",
    "for pil_img in interpolation_results:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "848fe3fc",
   "metadata": {},
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "output_file = open(\"./output.mp4\",\"rb\").read()\n",
    "file_data = f\"data:video/mp4;base64,{b64encode(output_file).decode()}\"\n",
    "HTML(f\"\"\"<video width=800 controls><source src=\"{file_data}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "include = [\n",
    "           \"a futuristic city in synthwave style\"\n",
    "          ]\n",
    "TOTAL_ITERATIONS = 500\n",
    "exclude =  \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"\"#\"watercolor paper texture\"\n",
    "\n",
    "w1=1\n",
    "w2=0.5\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True,\n",
    "                              capture_gen_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c2056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1]*len(result_z)\n",
    "interpolation_results = interpolate(result_z,durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = \"./outputs/output2.mp4\"\n",
    "writer = imageio.get_writer(output_video_path,fps=25)\n",
    "for pil_img in interpolation_results:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ca4adc4",
   "metadata": {},
   "source": [
    "output_file = open(\"./output2.mp4\",\"rb\").read()\n",
    "file_data = f\"data:video/mp4;base64,{b64encode(output_file).decode()}\"\n",
    "HTML(f\"\"\"<video width=800 controls><source src=\"{file_data}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a56390",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "include = [\n",
    "           \"a cyberpunk city in synthwave\"\n",
    "          ]\n",
    "TOTAL_ITERATIONS = 500\n",
    "exclude =  \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"\"#\"watercolor paper texture\"\n",
    "\n",
    "w1=1\n",
    "w2=1\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True,\n",
    "                              capture_gen_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1]*len(result_z)\n",
    "interpolation_results = interpolate(result_z,durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = \"./outputs/output3.mp4\"\n",
    "writer = imageio.get_writer(output_video_path,fps=25)\n",
    "for pil_img in interpolation_results:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28e00b32",
   "metadata": {},
   "source": [
    "output_file = open(\"./output3.mp4\",\"rb\").read()\n",
    "file_data = f\"data:video/mp4;base64,{b64encode(output_file).decode()}\"\n",
    "HTML(f\"\"\"<video width=800 controls><source src=\"{file_data}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80368c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "include = [\n",
    "           \"a piano\"\n",
    "          ]\n",
    "TOTAL_ITERATIONS = 500\n",
    "exclude =  \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"watercolor paper texture\"\n",
    "\n",
    "w1=1\n",
    "w2=1\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True,\n",
    "                              capture_gen_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f966b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1]*len(result_z)\n",
    "interpolation_results = interpolate(result_z,durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = \"./outputs/output4.mp4\"\n",
    "writer = imageio.get_writer(output_video_path,fps=25)\n",
    "for pil_img in interpolation_results:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9f108b2",
   "metadata": {},
   "source": [
    "output_file = open(\"./output4.mp4\",\"rb\").read()\n",
    "file_data = f\"data:video/mp4;base64,{b64encode(output_file).decode()}\"\n",
    "HTML(f\"\"\"<video width=800 controls><source src=\"{file_data}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "include = [\n",
    "           \"a peaceful lake\"\n",
    "          ]\n",
    "TOTAL_ITERATIONS = 800\n",
    "exclude =  \"watermark, cropped, confusing, incoherent, cut, blurry\"\n",
    "extras = \"oil painting\"#\"watercolor paper texture\"\n",
    "\n",
    "w1=1\n",
    "w2=1\n",
    "\n",
    "result_imgs, result_z = train(include, exclude, extras, params,optimizer,show_crop=True,\n",
    "                              capture_gen_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [1]*len(result_z)\n",
    "interpolation_results = interpolate(result_z,durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea628821",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_path = \"./outputs/output5.mp4\"\n",
    "writer = imageio.get_writer(output_video_path,fps=25)\n",
    "for pil_img in interpolation_results:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "833b64c1",
   "metadata": {},
   "source": [
    "output_file = open(\"./output5.mp4\",\"rb\").read()\n",
    "file_data = f\"data:video/mp4;base64,{b64encode(output_file).decode()}\"\n",
    "HTML(f\"\"\"<video width=800 controls><source src=\"{file_data}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251e3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
